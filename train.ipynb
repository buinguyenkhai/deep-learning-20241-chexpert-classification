{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-13T06:22:48.499426Z",
     "iopub.status.busy": "2024-12-13T06:22:48.499061Z",
     "iopub.status.idle": "2024-12-13T06:23:12.209810Z",
     "shell.execute_reply": "2024-12-13T06:23:12.208905Z",
     "shell.execute_reply.started": "2024-12-13T06:22:48.499380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import tensorflow as tf\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "wandb.login(key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T06:23:12.212760Z",
     "iopub.status.busy": "2024-12-13T06:23:12.211674Z",
     "iopub.status.idle": "2024-12-13T06:23:12.230106Z",
     "shell.execute_reply": "2024-12-13T06:23:12.229420Z",
     "shell.execute_reply.started": "2024-12-13T06:23:12.212716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CheXpertDataset(Dataset):\n",
    "    '''\n",
    "    Custom dataset for CheXpert. Returns a tuple of (PIL.Image, torch.Tensor (float32)).\n",
    "    Args:\n",
    "        data (pd.Dataframe): dataset with image path as indexes and columns as labels, all values are 0 (negative) or 1 (postivive)\n",
    "        root_dir (str): root directory of dataset folder\n",
    "        mode ('train', 'val'): mode for different augmentation\n",
    "        transforms (albumentations): augmentation techniques to use\n",
    "    '''\n",
    "    def __init__(self, data, root_dir, mode='train', transforms=None):\n",
    "        self.data = data.to_numpy()\n",
    "        self.labels = torch.tensor(data.values.astype(np.float32))\n",
    "        self.root_dir = root_dir\n",
    "        self.img_paths = [os.path.join(root_dir, img_path) for img_path in data.index]\n",
    "        self.transform = transforms.get(mode)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=np.array(image))['image']\n",
    "\n",
    "        return (image, label)\n",
    "    \n",
    "def get_weighted_sampler(data):\n",
    "    '''\n",
    "    Custom Sampler for weighted sampling to deal with unbalanced labels. The class weights are the inverse of the count of positives of a label.\n",
    "        Args:\n",
    "            data (pd.Dataframe): dataset with image path as indexes and columns as labels, all values are 0 (negative) or 1 (postivive)\n",
    "            batch_size (int): number of indices to use at a time \n",
    "    '''\n",
    "    class_weights = (1/data.sum()).values\n",
    "    weights = data.dot(class_weights)\n",
    "    weighted_sampler = WeightedRandomSampler(torch.tensor(weights.values, dtype=torch.float), len(weights), replacement=True)\n",
    "    return weighted_sampler\n",
    "'''\n",
    "Augmentations:\n",
    "    - Scale 5% with p = 50%\n",
    "    - Rotate 20Â° OR shear 5 pixels with p = 50%\n",
    "    - Translate 5% with p = 50%\n",
    "    - Resize to 224x224\n",
    "    - Normalize with mean = 0.506 and std = 0.287, more details in data_preprocessing.ipynb\n",
    "    - Convert to torch.Tensor\n",
    "'''\n",
    "transform = {\n",
    "    'train': A.Compose([\n",
    "        A.Affine(scale=(0.95, 1.05), p=0.5),\n",
    "        A.OneOf([A.Affine(rotate=(-20, 20), p=0.5), A.Affine(shear=(-5, 5), p=0.5)], p=0.5),\n",
    "        A.Affine(translate_percent=(-0.05, 0.05), p=0.5),\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize([0.506, 0.506, 0.506], [0.287, 0.287, 0.287]),\n",
    "        ToTensorV2()\n",
    "    ]),\n",
    "    'val': A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize([0.506, 0.506, 0.506], [0.287, 0.287, 0.287]),\n",
    "        ToTensorV2()\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T06:23:12.231687Z",
     "iopub.status.busy": "2024-12-13T06:23:12.231408Z",
     "iopub.status.idle": "2024-12-13T06:23:12.932230Z",
     "shell.execute_reply": "2024-12-13T06:23:12.931544Z",
     "shell.execute_reply.started": "2024-12-13T06:23:12.231649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/chexpertclean/u1_test.csv',index_col=0)\n",
    "train = pd.read_csv('/kaggle/input/chexpertclean/u1_train.csv', index_col=0)\n",
    "val = pd.read_csv('/kaggle/input/chexpertclean/u1_val.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T06:23:12.934200Z",
     "iopub.status.busy": "2024-12-13T06:23:12.933931Z",
     "iopub.status.idle": "2024-12-13T06:23:13.013334Z",
     "shell.execute_reply": "2024-12-13T06:23:13.012427Z",
     "shell.execute_reply.started": "2024-12-13T06:23:12.934175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train.index =  train.index.str.replace('CheXpert-v1.0-small', 'chexpert')\n",
    "test.index = test.index.str.replace('CheXpert-v1.0-small', 'chexpert')\n",
    "val.index = val.index.str.replace('CheXpert-v1.0-small', 'chexpert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm no data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T06:23:13.014898Z",
     "iopub.status.busy": "2024-12-13T06:23:13.014555Z",
     "iopub.status.idle": "2024-12-13T06:23:13.472549Z",
     "shell.execute_reply": "2024-12-13T06:23:13.471654Z",
     "shell.execute_reply.started": "2024-12-13T06:23:13.014865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(np.intersect1d(train.index.to_numpy(), test.index.to_numpy()))\n",
    "print(np.intersect1d(val.index.to_numpy(), test.index.to_numpy()))\n",
    "print(np.intersect1d(val.index.to_numpy(), train.index.to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration for Bayes Hyperparameter Tuning using Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T06:23:13.474161Z",
     "iopub.status.busy": "2024-12-13T06:23:13.473808Z",
     "iopub.status.idle": "2024-12-13T06:23:14.063783Z",
     "shell.execute_reply": "2024-12-13T06:23:14.062925Z",
     "shell.execute_reply.started": "2024-12-13T06:23:13.474125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"val_mAUC\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [1e-3, 1e-4, 1e-5]\n",
    "        },\n",
    "        \"weight_decay\": {\n",
    "            \"values\": [1e-3, 1e-4, 1e-5]\n",
    "        },\n",
    "        \"patience\": {\n",
    "            \"values\": [1, 3]\n",
    "        }#,\n",
    "        #\"drop_rate\": {\n",
    "        #    \"values\": [0, 0.25]\n",
    "        #}\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"deep_learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T06:23:14.524743Z",
     "iopub.status.busy": "2024-12-13T06:23:14.524413Z",
     "iopub.status.idle": "2024-12-13T06:23:14.536858Z",
     "shell.execute_reply": "2024-12-13T06:23:14.536146Z",
     "shell.execute_reply.started": "2024-12-13T06:23:14.524707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = CheXpertDataset(\n",
    "    data=train,\n",
    "    root_dir='/kaggle/input/', \n",
    "    mode='train',\n",
    "    transforms = transform\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler = get_weighted_sampler(train),\n",
    "    num_workers = 4,\n",
    "    pin_memory=True\n",
    "    )\n",
    "\n",
    "val_dataset = CheXpertDataset(\n",
    "    val, '/kaggle/input/',\n",
    "    mode='val',\n",
    "    transforms = transform)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = batch_size,\n",
    "    num_workers = 4,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T06:23:14.538159Z",
     "iopub.status.busy": "2024-12-13T06:23:14.537903Z",
     "iopub.status.idle": "2024-12-13T06:23:14.616222Z",
     "shell.execute_reply": "2024-12-13T06:23:14.615264Z",
     "shell.execute_reply.started": "2024-12-13T06:23:14.538136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        model = models.resnet152(num_classes=14).to(device)\n",
    "        loss_function = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=config.patience)\n",
    "        mAUC = tf.keras.metrics.AUC(multi_label=True, from_logits=True)\n",
    "        \n",
    "        best_val_mAUC = 0\n",
    "        epochs_without_improvement = 0\n",
    "        max_epochs = 50\n",
    "        val_patience = 5\n",
    "        for epoch in range(max_epochs):\n",
    "            \n",
    "            model.train()\n",
    "            epoch_loss = 0.0\n",
    "            for inputs, labels in tqdm((train_loader), desc=f\"Epoch {epoch+1}/{max_epochs}\", unit=\"batch\"):\n",
    "                # Compute prediction and loss\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "        \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "                # Accumulate loss\n",
    "                epoch_loss += loss.item()\n",
    "        \n",
    "            # Compute and print average loss for the epoch\n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "            scheduler.step(avg_loss)\n",
    "            print(f\"Epoch {epoch+1}/{max_epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "            model.eval()\n",
    "            mAUC.reset_state()\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in tqdm(val_loader, desc=\"Testing\", unit=\"batch\"):\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "                    # Get the logits from the model\n",
    "                    outputs = model(inputs)\n",
    "            \n",
    "\n",
    "            \n",
    "                    mAUC.update_state(labels.cpu().numpy(), outputs.cpu().numpy())\n",
    "        \n",
    "            val_mAUC = mAUC.result().numpy()\n",
    "            print(f'Mean AUROC: {val_mAUC:.4f}')\n",
    "        \n",
    "            wandb.log({\"epoch\": epoch, \"train_loss\": avg_loss, \"val_mAUC\": val_mAUC})\n",
    "            \n",
    "            if val_mAUC > best_val_mAUC:\n",
    "                best_val_mAUC = val_mAUC\n",
    "                torch.save(model.state_dict(), \"ResNetScratch-U1.pth\")\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                \n",
    "            # Early stopping\n",
    "            if epochs_without_improvement >= val_patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T06:23:14.618726Z",
     "iopub.status.busy": "2024-12-13T06:23:14.618347Z",
     "iopub.status.idle": "2024-12-13T06:26:23.456162Z",
     "shell.execute_reply": "2024-12-13T06:26:23.455137Z",
     "shell.execute_reply.started": "2024-12-13T06:23:14.618688Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T06:26:23.458308Z",
     "iopub.status.busy": "2024-12-13T06:26:23.457917Z",
     "iopub.status.idle": "2024-12-13T06:26:23.468055Z",
     "shell.execute_reply": "2024-12-13T06:26:23.467033Z",
     "shell.execute_reply.started": "2024-12-13T06:26:23.458255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate and plot AUROC for multi-label classification\n",
    "def plot_auroc(model, dataloader, num_classes, device):\n",
    "    model.eval()\n",
    "    \n",
    "    true_labels = []\n",
    "    predicted_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Get raw model output (logits)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Apply sigmoid to get probabilities for each label\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            \n",
    "            true_labels.append(labels.cpu().numpy())\n",
    "            predicted_probs.append(probs.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    true_labels = np.vstack(true_labels)\n",
    "    predicted_probs = np.vstack(predicted_probs)\n",
    "\n",
    "    # Calculate AUROC for each label\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "\n",
    "    # Iterate over each label and compute the ROC curve and AUROC\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(true_labels[:, i], predicted_probs[:, i])\n",
    "        roc_auc[i] = roc_auc_score(true_labels[:, i], predicted_probs[:, i])\n",
    "\n",
    "    # Plot the ROC curve for each label\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(num_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label=f'Class {i+1} (AUROC = {roc_auc[i]:.2f})')\n",
    "    \n",
    "    # Plot the diagonal (random guess line)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random guess (AUROC = 0.5)')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.title('Multi-label (ROC) Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return mean AUROC\n",
    "    mean_auroc = np.mean(list(roc_auc.values()))\n",
    "    print(f'Mean AUROC across all classes: {mean_auroc:.2f}')\n",
    "    return mean_auroc\n",
    "\n",
    "#model.to(device)\n",
    "#plot_auroc(model, val_loader, num_classes=14, device='cuda')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1302315,
     "sourceId": 2169393,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6290422,
     "sourceId": 10182897,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
